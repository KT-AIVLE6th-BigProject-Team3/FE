# ê¸°ë³¸ Python ë¼ì´ë¸ŒëŸ¬ë¦¬
import os
import sys
import json
import requests
import xml.etree.ElementTree as ET
 
# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import pandas as pd
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
 
# OpenAI ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
import openai
from openai import OpenAI
 
# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_community.document_loaders import TextLoader
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.chat_models import ChatOpenAI

import joblib 
from datetime import datetime
from langchain.chains import LLMChain
import pdfkit

from fastapi.responses import FileResponse



 
# FastAPI ì•± ìƒì„±
app = FastAPI()
 
# CORS ë¯¸ë“¤ì›¨ì–´ ì„¤ì • (í”„ë¡ íŠ¸ì—”ë“œì™€ í†µì‹ ì„ ìœ„í•´ í•„ìš”?)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
 
# ì „ì—­ ë³€ìˆ˜
qa_chain = None
openai_api_key = None
 
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def load_file(filepath):
    with open(filepath, 'r') as file:
        return file.readline().strip()


def load_csv(file_path):
    try:
        df = pd.read_csv(file_path, encoding="euc-kr")
        print(f"Loaded {len(df)} rows and {len(df.columns)} columns.")
        
        return df
    except UnicodeDecodeError:
        raise ValueError("Error: Unable to decode the file. Please check the file encoding.")


 
def extract_text_data(df):
    if {'êµ¬ë¶„','ë‚´ìš©'}.issubset(df.columns):
        return [f"êµ¬ë¶„: {row['êµ¬ë¶„']}\në‚´ìš©: {row['ë‚´ìš©']}" for _, row in df.iterrows()]
    else:
        raise ValueError("The required columns ('êµ¬ë¶„', 'ë‚´ìš©') are not found in the CSV file.")

def split_texts(text_data, chunk_size=1000, chunk_overlap=100):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    split_docs = []
    for text in text_data:
        split_docs.extend(text_splitter.split_text(text))
    print(f"Split into {len(split_docs)} chunks.")
    return split_docs
 
def create_vectorstore(split_docs, embeddings_model):
    try:
        vectorstore = FAISS.from_texts(split_docs, embeddings_model)
        print("Embeddings created and vectorstore generated.")
        return vectorstore
    except Exception as e:
        raise RuntimeError(f"Error during embeddings creation: {e}")
 
def save_vectorstore(vectorstore, path):
    vectorstore.save_local(path)
    print(f"Vectorstore saved locally as '{path}'.")
 
def load_vectorstore(path, embeddings_model):
    return FAISS.load_local(path, embeddings_model, allow_dangerous_deserialization=True)
 
def setup_qa_chain(vectorstore):
    prompt = PromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ì§ˆë¬¸-ë‹µë³€(Question-Answering)ì„ ìˆ˜í–‰í•˜ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ì–´ì§„ ë¬¸ë§¥(context) ì—ì„œ ì£¼ì–´ì§„ ì§ˆë¬¸(question) ì— ë‹µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
        ê²€ìƒ‰ëœ ë‹¤ìŒ ë¬¸ë§¥(context) ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸(question) ì— ë‹µí•˜ì„¸ìš”. ë§Œì•½, ì£¼ì–´ì§„ ë¬¸ë§¥(context) ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, ë‹µì„ ëª¨ë¥¸ë‹¤ë©´ `ì£¼ì–´ì§„ ì •ë³´ì—ì„œ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤` ë¼ê³  ë‹µí•˜ì„¸ìš”.
        í•œê¸€ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”. ë‹¨, ê¸°ìˆ ì ì¸ ìš©ì–´ë‚˜ ì´ë¦„ì€ ë²ˆì—­í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ ì£¼ì„¸ìš”. 
 
        #Question:
        {question}
 
        #Context:
        {context}
 
        #Answer:  (ìì„¸í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”)""" 
    )
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)

    
    return RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectorstore.as_retriever(),
        chain_type_kwargs={"prompt": prompt},
        return_source_documents=True
    )
 
# ì•± ì‹œì‘ ì‹œ ì´ˆê¸°í™”
@app.on_event("startup")
async def startup_event():
    global qa_chain, openai_api_key
    
    try:
        # API í‚¤ ë¡œë“œ ë° í™˜ê²½ë³€ìˆ˜ ì„¤ì •
        openai_api_key = load_file('api_key.txt')
        os.environ['OPENAI_API_KEY'] = openai_api_key
 
        # CSV ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
        file_path = "example.csv"
        df = load_csv(file_path)
        text_data = extract_text_data(df)
 
        # í…ìŠ¤íŠ¸ ë¶„í• 
        split_docs = split_texts(text_data)
 
        # ì„ë² ë”© ëª¨ë¸ ë° ë²¡í„°ìŠ¤í† ì–´ ì„¤ì •
        embeddings_model = OpenAIEmbeddings(openai_api_key=openai_api_key)
        vectorstore = create_vectorstore(split_docs, embeddings_model)
        save_vectorstore(vectorstore, "faiss_index")
 
        # ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ë° QA ì²´ì¸ ì„¤ì •
        vectorstore = load_vectorstore("faiss_index", embeddings_model)
        qa_chain = setup_qa_chain(vectorstore)
        
        print("Application successfully initialized!")
    except Exception as e:
        print(f"Error during initialization: {e}")
        sys.exit(1)
 
# API ì—”ë“œí¬ì¸íŠ¸
@app.get('/')
def read_root():
    return {"message": "Welcome to QA Chatbot API!"}
 

@app.get("/chat-bot")
async def chat_endpoint(question: str):
    global qa_chain
    try:
        if qa_chain is None:
            raise HTTPException(status_code=500, detail="QA chain not initialized")
        
        response = qa_chain.invoke({"query": question})
        
        return response["result"]
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
    

# ğŸ”¥ PDF ì €ì¥ ê²½ë¡œ ì„¤ì •
REPORTS_DIR = "reports"
os.makedirs(REPORTS_DIR, exist_ok=True)  # ğŸ“‚ í´ë”ê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±

# âœ… PDF ìƒì„± ì„¤ì •
PDFKIT_CONFIG = pdfkit.configuration(wkhtmltopdf=r"C:\Program Files\wkhtmltopdf\bin\wkhtmltopdf.exe")
    


# âœ… 1ï¸âƒ£ ì¥ë¹„ ë°ì´í„° ë¡œë“œ
def load_equipment_data():
    try:
        file_path = "agv_dataframe_version_2.pkl"
        df = joblib.load(file_path)
        print(f"âœ… ì¥ë¹„ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)} ê°œì˜ ë°ì´í„°")
        return df
    except Exception as e:
        raise ValueError(f"âŒ ì¥ë¹„ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")

# âœ… 2ï¸âƒ£ ì‚¬ìš©ì ì…ë ¥ ë°ì´í„° ëª¨ë¸ (MM-DD í˜•ì‹)
class EquipmentReportRequest(BaseModel):
    ì¥ë¹„_ID: str
    ì‹œì‘_ë‚ ì§œ: str  # MM-DD í˜•ì‹
    ì¢…ë£Œ_ë‚ ì§œ: str  # MM-DD í˜•ì‹

# âœ… 3ï¸âƒ£ ë‚ ì§œ ë³€í™˜ (MM-DD â†’ 2024ë…„ YYYY-MM-DD)
def convert_to_2024_date(mm_dd: str):
    return datetime.strptime(f"2024-{mm_dd}", "%Y-%m-%d")

# âœ… 4ï¸âƒ£ ì¥ë¹„ ë°ì´í„° ì¡°íšŒ
def fetch_equipment_data(ì¥ë¹„_ID, ì‹œì‘_ë‚ ì§œ, ì¢…ë£Œ_ë‚ ì§œ):
    df = load_equipment_data()

    if ì¥ë¹„_ID not in df["device_id"].unique():
        raise HTTPException(status_code=404, detail=f"âŒ ì¥ë¹„ ID '{ì¥ë¹„_ID}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    ì‹œì‘ = convert_to_2024_date(ì‹œì‘_ë‚ ì§œ)
    ì¢…ë£Œ = convert_to_2024_date(ì¢…ë£Œ_ë‚ ì§œ)

    df["collection_date"] = df["collection_date"].apply(lambda x: f"2024-{x}")
    df["collection_date"] = pd.to_datetime(df["collection_date"], format="%Y-%m-%d")

    filtered_df = df[
        (df["device_id"] == ì¥ë¹„_ID) & 
        (df["collection_date"] >= ì‹œì‘) & 
        (df["collection_date"] <= ì¢…ë£Œ)
    ]

    if filtered_df.empty:
        raise HTTPException(status_code=404, detail="âŒ í•´ë‹¹ ê¸°ê°„ì˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    return filtered_df.to_dict(orient="records")

# âœ… 5ï¸âƒ£ AI ê¸°ë°˜ ì¥ë¹„ ë³´ê³ ì„œ ìƒì„±
def create_ai_equipment_report(ì¥ë¹„_ID, ì‹œì‘_ë‚ ì§œ, ì¢…ë£Œ_ë‚ ì§œ, equipment_data):
    data_summary = "\n".join([
        f"- {d['collection_date']} {d['collection_time']}: ì˜¨ë„ {d['ex_temperature']}Â°C, ìŠµë„ {d['ex_humidity']}%, ì¡°ë„ {d['ex_illuminance']} lx, ìƒíƒœ {d['state']}"
        for d in equipment_data[:50]
    ])

    report_prompt = PromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ìš´ì†¡ ì¥ë¹„ ì •ë¹„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ëŠ” AIì…ë‹ˆë‹¤. 
        ì•„ë˜ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ìš”ì•½í•˜ì—¬ ì •ë¹„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

        # ì¥ë¹„ ID:
        {ì¥ë¹„_ID}

        # ê¸°ê°„:
        {ì‹œì‘_ë‚ ì§œ} ~ {ì¢…ë£Œ_ë‚ ì§œ}

        # ì¥ë¹„ ìƒíƒœ ë°ì´í„° ìš”ì•½:
        {data_summary}

        # ë³´ê³ ì„œ:
        """
    )

    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
    chain = LLMChain(llm=llm, prompt=report_prompt)
    report_text = chain.invoke({
        "ì¥ë¹„_ID": ì¥ë¹„_ID,
        "ì‹œì‘_ë‚ ì§œ": ì‹œì‘_ë‚ ì§œ,
        "ì¢…ë£Œ_ë‚ ì§œ": ì¢…ë£Œ_ë‚ ì§œ,
        "data_summary": data_summary
    })
    
    return report_text["text"]


# âœ… 1ï¸âƒ£ PDF ë³´ê³ ì„œ ìƒì„±
def generate_pdf_equipment_report(ì¥ë¹„_ID, ì‹œì‘_ë‚ ì§œ, ì¢…ë£Œ_ë‚ ì§œ, equipment_data, report_text):
    pdf_filename = f"{ì¥ë¹„_ID}_ì •ë¹„ë³´ê³ ì„œ.pdf"
    pdf_path = os.path.join(REPORTS_DIR, pdf_filename)  # âœ… `reports/` í´ë”ì— ì €ì¥

    html_content = f"""
    <html>
    <head><meta charset="UTF-8"></head>
    <body>
        <h1>{ì¥ë¹„_ID} ì •ë¹„ ë³´ê³ ì„œ</h1>
        <p><strong>ìš´ì˜ ê¸°ê°„:</strong> {ì‹œì‘_ë‚ ì§œ} ~ {ì¢…ë£Œ_ë‚ ì§œ}</p>

        <h2>ğŸ“Œ ì¥ë¹„ ë°ì´í„° ìš”ì•½</h2>
        <ul>
            {''.join([f"<li>{d['collection_date']} {d['collection_time']}: ì˜¨ë„ {d['ex_temperature']}Â°C, ìŠµë„ {d['ex_humidity']}%, ì¡°ë„ {d['ex_illuminance']} lx, ìƒíƒœ {d['state']}</li>" for d in equipment_data])}
        </ul>

        <h2>ğŸ“Œ AI ë¶„ì„ ë³´ê³ </h2>
        <p>{report_text}</p>
    </body>
    </html>
    """

    pdfkit.from_string(html_content, pdf_path, configuration=PDFKIT_CONFIG)
    print(f"âœ… PDF ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {pdf_path}")  # ë””ë²„ê¹…ìš© ë¡œê·¸ ì¶”ê°€
    return pdf_path

from fastapi.responses import FileResponse
import os

# âœ… ì¥ë¹„ ë³´ê³ ì„œ ìƒì„± í›„ PDF ìƒì„±í•˜ì§€ ì•Šê³  `download-report` ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ìƒì„±
@app.post("/generate-equipment-report")
async def generate_equipment_report(request: EquipmentReportRequest):
    try:
        equipment_data = fetch_equipment_data(request.ì¥ë¹„_ID, request.ì‹œì‘_ë‚ ì§œ, request.ì¢…ë£Œ_ë‚ ì§œ)
        report_text = create_ai_equipment_report(request.ì¥ë¹„_ID, request.ì‹œì‘_ë‚ ì§œ, request.ì¢…ë£Œ_ë‚ ì§œ, equipment_data)
        pdf_filename = generate_pdf_equipment_report(request.ì¥ë¹„_ID, request.ì‹œì‘_ë‚ ì§œ, request.ì¢…ë£Œ_ë‚ ì§œ, equipment_data, report_text)
        return {"message": "ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ", "pdf_filename": pdf_filename}
    except Exception as e:
        return {"error": str(e)}

# âœ… ì‚¬ìš©ìê°€ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì„ í´ë¦­í•  ë•Œ PDFë¥¼ ìƒì„±í•˜ê³  ë°˜í™˜
# âœ… íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ë°˜í™˜, ì—†ìœ¼ë©´ ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥
@app.get("/download-report/{filename}")
async def download_report(filename: str):
    file_path = os.path.join(REPORTS_DIR, filename)
    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return FileResponse(file_path, media_type='application/pdf', filename=filename)